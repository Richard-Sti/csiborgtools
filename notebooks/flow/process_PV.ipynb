{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process PV catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from h5py import File\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "SPEED_OF_LIGHT = 299_792.458"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supernovae data\n",
    "\n",
    "### LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2dir = \"/mnt/extraspace/rstiskalek/catalogs/PV/PV_Supranta/A2\"\n",
    "\n",
    "names = [\"z_CMB\", \"mB\", \"x1\", \"c\", \"e_mB\", \"e_x1\", \"e_c\", \"RA\", \"DEC\"]\n",
    "dtype = [(n, np.float32) for n in names]\n",
    "data = np.genfromtxt(join(a2dir, \"loss.csv\"), delimiter=\",\", skip_header=1,\n",
    "                     usecols=[5 + n for n in range(len(names))])\n",
    "\n",
    "loss_data = np.empty(len(data), dtype=dtype)\n",
    "for i, n in enumerate(names):\n",
    "    loss_data[n] = data[:, i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"z_CMB\", \"RA\", \"DEC\", \"x1\", \"mB\", \"c\", \"peak\", \"e_peak\", \"e_x1\", \"e_mB\", \"e_c\"]\n",
    "dtype = [(n, np.float32) for n in names]\n",
    "data = np.genfromtxt(join(a2dir, \"foundation.csv\"), delimiter=\",\", skip_header=1,\n",
    "                     usecols=[3 + n for n in range(len(names))])\n",
    "\n",
    "foundation_data = np.empty(len(data), dtype=dtype)\n",
    "for i, n in enumerate(names):\n",
    "    foundation_data[n] = data[:, i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pantheon+, all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"/mnt/extraspace/rstiskalek/catalogs/PV/Pantheon+SH0ES.dat\"\n",
    "\n",
    "data = np.genfromtxt(fpath, names=True, dtype=None, encoding=None)\n",
    "data = data[data[\"zCMB\"] < 0.1]\n",
    "\n",
    "keys = [\"zCMB\", \"mB\", \"mBERR\", \"x1\", \"x1ERR\", \"c\", \"cERR\", \"RA\", \"DEC\",\n",
    "        \"VPEC\", \"VPECERR\", \"biasCor_m_b\", \"biasCorErr_m_b\"]\n",
    "\n",
    "pantheon_data = data[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf_folder = \"/Users/richard/Data/PV/A2_paper_data/tf\"\n",
    "\n",
    "names = [\"RA\", \"DEC\", \"z_CMB\", \"mag\", \"eta\", \"e_mag\", \"e_eta\"]\n",
    "\n",
    "dtype = [(n, np.float32) for n in names]\n",
    "data = np.genfromtxt(join(tf_folder, \"sfi_gals_tf.csv\"), delimiter=\",\", skip_header=1,\n",
    "                     usecols=[2 + n for n in range(len(names))])\n",
    "\n",
    "sfi_gals = np.empty(len(data), dtype=dtype)\n",
    "for i, n in enumerate(names):\n",
    "    sfi_gals[n] = data[:, i]\n",
    "\n",
    "names = [\"RA\", \"DEC\", \"z_CMB\", \"mag\", \"eta\", \"e_mag\", \"e_eta\"]\n",
    "\n",
    "dtype = [(n, np.float32) for n in names]\n",
    "data = np.genfromtxt(join(tf_folder, \"sfi_gals_tf_masked.csv\"), delimiter=\",\", skip_header=1,\n",
    "                     usecols=[2 + n for n in range(len(names))])\n",
    "\n",
    "sfi_gals_masked = np.empty(len(data), dtype=dtype)\n",
    "for i, n in enumerate(names):\n",
    "    sfi_gals_masked[n] = data[:, i]\n",
    "\n",
    "names = [\"RA\", \"DEC\", \"z_CMB\", \"r_hMpc\", \"e_r_hMpc\"]\n",
    "\n",
    "dtype = [(n, np.float32) for n in names]\n",
    "data = np.genfromtxt(join(tf_folder, \"sfi_grps.csv\"), delimiter=\",\", skip_header=1,\n",
    "                     usecols=[1 + n for n in range(len(names))])\n",
    "\n",
    "sfi_groups = np.empty(len(data), dtype=dtype)\n",
    "for i, n in enumerate(names):\n",
    "    sfi_groups[n] = data[:, i]\n",
    "\n",
    "names = [\"RA\", \"DEC\", \"mag\", \"e_mag\", \"z_CMB\", \"r_hMpc\", \"e_rhMpc\", \"M\", \"eta\", \"e_eta\"]\n",
    "\n",
    "dtype = [(n, np.float32) for n in names]\n",
    "data = np.genfromtxt(join(tf_folder, \"twomtf_k.csv\"), delimiter=\",\", skip_header=1,\n",
    "                     usecols=[2 + n for n in range(len(names))])\n",
    "\n",
    "twomtf_gals = np.empty(len(data), dtype=dtype)\n",
    "for i, n in enumerate(names):\n",
    "    twomtf_gals[n] = data[:, i]\n",
    "\n",
    "outdir = \"/Users/richard/Downloads\"\n",
    "fname = \"PV_compilation_Supranta2019.hdf5\"\n",
    "\n",
    "with File(join(outdir, fname), 'w') as f:\n",
    "    # Write LOSS\n",
    "    grp = f.create_group(\"LOSS\")\n",
    "    for name in loss_data.dtype.names:\n",
    "        grp.create_dataset(name, data=loss_data[name])\n",
    "\n",
    "    # Write Foundation\n",
    "    grp = f.create_group(\"Foundation\")\n",
    "    for name in foundation_data.dtype.names:\n",
    "        grp.create_dataset(name, data=foundation_data[name])\n",
    "\n",
    "    # Write SFI gals\n",
    "    grp = f.create_group(\"SFI_gals\")\n",
    "    for name in sfi_gals.dtype.names:\n",
    "        grp.create_dataset(name, data=sfi_gals[name])\n",
    "    \n",
    "    # Write SFI gals masked\n",
    "    grp = f.create_group(\"SFI_gals_masked\")\n",
    "    for name in sfi_gals_masked.dtype.names:\n",
    "        grp.create_dataset(name, data=sfi_gals_masked[name])\n",
    "\n",
    "    # Write SFI groups\n",
    "    grp = f.create_group(\"SFI_groups\")\n",
    "    for name in sfi_groups.dtype.names:\n",
    "        grp.create_dataset(name, data=sfi_groups[name])\n",
    "\n",
    "    # Write 2MTF gals\n",
    "    grp = f.create_group(\"2MTF\")\n",
    "    for name in twomtf_gals.dtype.names:\n",
    "        grp.create_dataset(name, data=twomtf_gals[name])\n",
    "\n",
    "    # Write Pantheon\n",
    "    grp = f.create_group(\"Pantheon+\")\n",
    "    for name in pantheon_data.dtype.names:\n",
    "        grp.create_dataset(name, data=pantheon_data[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/mnt/extraspace/rstiskalek/catalogs/PV/PV_Supranta/A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pantheon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['2MTF', 'Foundation', 'LOSS', 'Pantheon+', 'SFI_gals', 'SFI_gals_masked', 'SFI_groups']>\n"
     ]
    }
   ],
   "source": [
    "# /mnt/extraspace/rstiskalek/catalogs/pantheon+_groups.hdf5\n",
    "\n",
    "\n",
    "with File(\"/mnt/extraspace/rstiskalek/catalogs/PV_compilation_Supranta2019.hdf5\", 'r') as f:\n",
    "    print(f.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_csiborg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
